{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-21T14:07:45.709922Z",
     "start_time": "2025-09-21T14:07:45.707323Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from einops import rearrange, repeat\n",
    "from pandas import Series\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T14:07:45.728259Z",
     "start_time": "2025-09-21T14:07:45.720029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ctx = Series()\n",
    "ctx.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "c2654bb26dbf0a3a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T14:07:45.738709Z",
     "start_time": "2025-09-21T14:07:45.732801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, pdrop=0.2):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.nhead = nhead\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.drop = nn.Dropout(pdrop)\n",
    "        self.c_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        b, t, c = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda tsn: rearrange(tsn, 'b t (nh hd) -> b nh t hd', nh=self.nhead), qkv)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * k.size(dim=-1) ** -0.5\n",
    "        if attn_mask is not None:\n",
    "            wei = wei.masked_fill(attn_mask[:t, :t] == 0, float('-inf'))\n",
    "        attn = self.attend(wei)\n",
    "\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'b nh t hd -> b t (nh hd)', nh=self.nhead)\n",
    "        return self.drop(self.c_proj(y))\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, embed_dim, pdrop=0.2):\n",
    "        super(FFN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, pdrop=0.2):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, nhead, pdrop)\n",
    "        self.ffn = FFN(embed_dim, pdrop)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = x + self.attn(self.ln_1(x), attn_mask=attn_mask)\n",
    "        x = x + self.ffn(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class StackBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, n_layer, pdrop=0.2):\n",
    "        super(StackBlock, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embed_dim, nhead, pdrop) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)"
   ],
   "id": "2fe2863905c98a3b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T14:07:45.746656Z",
     "start_time": "2025-09-21T14:07:45.742419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_dim, n_layer, nhead, pdrop, output_dim):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_num = img_size // patch_size\n",
    "        scale = embed_dim ** -0.5\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_embed = nn.Parameter(torch.randn(embed_dim) * scale)\n",
    "        self.pos_embed = nn.Parameter(scale * torch.randn(self.patch_num ** 2 + 1, embed_dim))\n",
    "        self.ln_pre = nn.LayerNorm(embed_dim)\n",
    "        self.transformer = StackBlock(\n",
    "            embed_dim, nhead, n_layer, pdrop\n",
    "        )\n",
    "        self.ln_post = nn.LayerNorm(embed_dim)\n",
    "        self.proj = nn.Parameter(torch.randn(embed_dim, output_dim) * scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        x = torch.cat(\n",
    "            (repeat(self.cls_embed, 'c -> b 1 c', b=x.shape[0]), x), dim=1\n",
    "        )\n",
    "        x += self.pos_embed\n",
    "        x = self.ln_pre(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "        return x @ self.proj"
   ],
   "id": "582be29ccd72ff4a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T14:07:45.772770Z",
     "start_time": "2025-09-21T14:07:45.751114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VisionTransformer(\n",
    "    img_size=32, patch_size=4, embed_dim=192, n_layer=6, nhead=3, pdrop=0.1, output_dim=10\n",
    ").to(ctx.device)"
   ],
   "id": "dce4a44d3f650787",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T14:07:47.272616Z",
     "start_time": "2025-09-21T14:07:45.777046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomErasing(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='~/data', train=True, download=True, transform=train_transform)\n",
    "test_data = datasets.CIFAR10(root='~/data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "classes = train_data.classes\n",
    "\n",
    "ctx.batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=ctx.batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=ctx.batch_size, shuffle=True)"
   ],
   "id": "76495b4e706b15d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T14:26:16.786341Z",
     "start_time": "2025-09-21T14:07:47.310989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ctx.epochs = 32\n",
    "ctx.lr = 1e-4\n",
    "ctx.eval_interval = 1\n",
    "ctx.save_weight = 'vit.pt'\n",
    "ctx.weight_decay = 0.1\n",
    "\n",
    "if os.path.exists(ctx.save_weight):\n",
    "    model.load_state_dict(torch.load(ctx.save_weight, weights_only=True))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=ctx.lr, weight_decay=ctx.weight_decay)\n",
    "\n",
    "pbar = tqdm(range(1, 1 + ctx.epochs))\n",
    "len_train_dl = len(train_dataloader)\n",
    "len_test_dl = len(test_dataloader)\n",
    "for epoch in pbar:\n",
    "    train_avg_loss = 0\n",
    "    model.train()\n",
    "    for idx, (imgs, labels) in enumerate(train_dataloader):\n",
    "        loss = criterion(model(imgs.to(ctx.device)), labels.to(ctx.device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_avg_loss += loss.item() / len_train_dl\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix()\n",
    "        pbar.set_postfix_str(f'{idx + 1}/{len_train_dl}')\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        ctx.save_weight\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        if epoch % ctx.eval_interval == 0:\n",
    "            model.eval()\n",
    "            val_avg_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for imgs, labels in test_dataloader:\n",
    "                logits = model(imgs.to(ctx.device))\n",
    "                val_avg_loss += criterion(logits, labels.to(ctx.device)).item() / len_test_dl\n",
    "                total += labels.size(0)\n",
    "                pred_labels = logits.argmax(dim=-1)\n",
    "                correct += torch.sum(\n",
    "                    pred_labels == labels.to(ctx.device)\n",
    "                )\n",
    "            pbar.set_description_str(f'train_loss: {train_avg_loss:.4f}, test loss: {val_avg_loss:.4f}, test acc: {correct / total:.4f}')"
   ],
   "id": "ef60776a5a390327",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6341, test loss: 0.6121, test acc: 0.7919: 100%|██████████| 32/32 [18:29<00:00, 34.67s/it, 782/782]\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
