{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-20T15:43:15.487525Z",
     "start_time": "2025-09-20T15:43:15.478269Z"
    }
   },
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat"
   ],
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:43:15.514879Z",
     "start_time": "2025-09-20T15:43:15.503294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, pdrop):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.c_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.drop = nn.Dropout(pdrop)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        _, t, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda tsn: rearrange(tsn, 'b t (nh hd) -> b nh t hd', nh=self.nhead), qkv\n",
    "        )\n",
    "\n",
    "        wei = q @ k.transpose(-1, -2) * k.size(dim=-1) ** -0.5\n",
    "        if attn_mask is not None:\n",
    "            wei = wei.masked_fill(attn_mask[:t, :t] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        attn = wei @ v\n",
    "        attn = rearrange(attn, 'b nh t hd -> b t (nh hd)')\n",
    "\n",
    "        return self.drop(self.c_proj(attn))"
   ],
   "id": "1d444c32dbb966",
   "outputs": [],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:43:15.525969Z",
     "start_time": "2025-09-20T15:43:15.519308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embed_dim, pdrop):\n",
    "        super(FFN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 # Common\n",
    "                 embed_dim,\n",
    "                 # MHA\n",
    "                 nhead,\n",
    "                 attn_pdrop,\n",
    "                 # FFN\n",
    "                 ffn_pdrop\n",
    "                 ):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, nhead, attn_pdrop)\n",
    "        self.ffn = FFN(embed_dim, ffn_pdrop)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = x + self.attn(self.ln1(x), attn_mask=attn_mask)\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ],
   "id": "dbe74fa6da45203f",
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:43:15.553324Z",
     "start_time": "2025-09-20T15:43:15.531849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 # LM\n",
    "                 vocab_size,\n",
    "                 ctx_len,\n",
    "                 # Common\n",
    "                 embed_dim,\n",
    "                 # Block\n",
    "                 nlayer,\n",
    "                 nhead,\n",
    "                 attn_mask=None,\n",
    "                 attn_pdrop=0.2,\n",
    "                 ffn_pdrop=0.2,\n",
    "                 # Output\n",
    "                 output_dim: int=None\n",
    "                 ):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        # Properties\n",
    "        self.ctx_len = ctx_len\n",
    "        if attn_mask is None:\n",
    "            attn_mask = torch.tril(torch.ones(ctx_len, ctx_len))\n",
    "        self.attn_mask = attn_mask\n",
    "        self.understanding = output_dim is not None\n",
    "\n",
    "        # Comps\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(ctx_len, embed_dim)\n",
    "\n",
    "        self.ln_pre = nn.LayerNorm(embed_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(embed_dim, nhead, attn_pdrop, ffn_pdrop) for _ in range(nlayer)]\n",
    "        )\n",
    "        self.ln_post = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size if output_dim is None else output_dim)\n",
    "\n",
    "    def generate(self, idx, attn_mask=None, max_new_tokens=100, temp=1.0, topk=None):\n",
    "        if self.understanding:\n",
    "            raise NotImplementedError('Not for generating')\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.ctx_len:]\n",
    "            logits = self(idx_cond, attn_mask)\n",
    "            logits_of_last_tick = logits[:, -1, :] / temp\n",
    "            if topk is not None:\n",
    "                vals, _ = torch.topk(logits_of_last_tick, topk)\n",
    "                logits_of_last_tick[logits_of_last_tick < vals[-1]] = float('-inf')\n",
    "            if temp == 0:\n",
    "                idx_next = torch.argmax(logits_of_last_tick, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                probs = logits_of_last_tick.softmax(dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # cat on 'time' dimension\n",
    "        return idx\n",
    "\n",
    "    def forward(self, idx, attn_mask=None):\n",
    "        attn_mask = attn_mask if attn_mask is not None else self.attn_mask\n",
    "\n",
    "        _, t = idx.shape\n",
    "\n",
    "        tok_emb = self.tok_emb(idx)\n",
    "        pos_emb = self.pos_emb(\n",
    "            torch.arange(0, t, dtype=torch.long, device=idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.ln_pre(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "        x = self.ln_post(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if self.understanding:\n",
    "            logits = logits[torch.arange(logits.shape[0]), idx.argmax(dim=-1)]\n",
    "\n",
    "        return logits"
   ],
   "id": "b70d11f8eff20f05",
   "outputs": [],
   "execution_count": 320
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:43:15.570321Z",
     "start_time": "2025-09-20T15:43:15.556997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pair(sz: Union[int, tuple]):\n",
    "    if isinstance(sz, int):\n",
    "        return sz, sz\n",
    "    else:\n",
    "        return sz\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 # Vision\n",
    "                 img_size: Union[int, tuple],\n",
    "                 patch_num: Union[int, tuple],\n",
    "                 use_cls_tok: bool,\n",
    "                 # Transformer\n",
    "                 nlayer: int,\n",
    "                 embed_dim: int,\n",
    "                 nhead: int,\n",
    "                 attn_pdrop: float,\n",
    "                 ffn_pdrop: float,\n",
    "                 # Output\n",
    "                 num_classes: int\n",
    "                 ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        # Properties\n",
    "        img_height, img_width = pair(img_size)\n",
    "        patch_height_n, patch_width_n = pair(patch_num)\n",
    "        scale = embed_dim ** -0.5\n",
    "\n",
    "        self.patch_height, self.patch_width = img_height // patch_height_n, img_width // patch_width_n\n",
    "        self.use_cls_tok = use_cls_tok\n",
    "\n",
    "        # Comps\n",
    "        self.to_patch = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=(patch_height_n, patch_width_n),\n",
    "            stride=(patch_height_n, patch_width_n),\n",
    "        )\n",
    "\n",
    "        self.pos_emb = nn.Parameter(scale * torch.randn(1, self.patch_height * self.patch_width + 1, embed_dim))\n",
    "        self.cls_emb = nn.Parameter(scale *  torch.randn(embed_dim))\n",
    "\n",
    "        self.ln_pre = nn.LayerNorm(embed_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(embed_dim, nhead, attn_pdrop, ffn_pdrop) for _ in range(nlayer)]\n",
    "        )\n",
    "        self.ln_post = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.proj = nn.Parameter(scale * torch.randn(embed_dim, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.to_patch(x)\n",
    "\n",
    "        idx = rearrange(\n",
    "            patches, 'b c (ph h) (pw w) -> b (ph pw) (c h w)', ph=self.patch_height, pw=self.patch_width\n",
    "        )\n",
    "\n",
    "\n",
    "        idx = torch.cat(\n",
    "            (\n",
    "                idx,\n",
    "                repeat(self.cls_emb, 'e -> b 1 e', b=idx.shape[0])\n",
    "            ), dim=1\n",
    "        )\n",
    "        x = idx + self.pos_emb\n",
    "\n",
    "\n",
    "        x = self.ln_pre(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        if self.use_cls_tok:\n",
    "            x = x[:, 0, :]\n",
    "        else:\n",
    "            x = x[:, 1:, :].mean(dim=1)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "\n",
    "        return x @ self.proj"
   ],
   "id": "af41ab8dcba651a3",
   "outputs": [],
   "execution_count": 321
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:43:15.579778Z",
     "start_time": "2025-09-20T15:43:15.573968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_encoder,\n",
    "                 text_encoder,\n",
    "                 ):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.img_encoder = img_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def encode_img(self, img):\n",
    "        return self.img_encoder(img)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        return self.text_encoder(text)\n",
    "\n",
    "    def forward(self, imgs, texts):\n",
    "        img_features = self.encode_img(imgs)\n",
    "        text_features = self.encode_text(texts)\n",
    "\n",
    "        if img_features.shape != text_features.shape:\n",
    "            raise ValueError(f'img_features shape {img_features.shape} != text_features shape {text_features.shape}')\n",
    "\n",
    "        img_features = img_features / img_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        logits_per_img = img_features @ text_features.t() * self.logit_scale.exp()\n",
    "        logits_per_text = logits_per_img.t()\n",
    "\n",
    "        return logits_per_img, logits_per_text"
   ],
   "id": "30c2205b0d61c16c",
   "outputs": [],
   "execution_count": 322
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
